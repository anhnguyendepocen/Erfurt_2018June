---
title: "Machine Learning"
subtitle: ""
author: "The R Bootcamp @ Erfurt<br/><a href='https://therbootcamp.github.io'>www.therbootcamp.com</a><br/><a href='https://twitter.com/therbootcamp'>@therbootcamp</a>"
date: "June 2018"
output:
  xaringan::moon_reader:
    css: ["default", "../_css/my-theme.css"]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: '16:9'
---

layout: true

<div class="my-footer"><span>
<a href="https://therbootcamp.github.io/"><font color="#7E7E7E">Erfurt, June 2018</font></a>
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;
&emsp;&emsp;&emsp;&emsp;&emsp;
<a href="https://therbootcamp.github.io/"><font color="#646464">www.therbootcamp.com</font></a>
</span></div> 

---
  
```{r, eval = FALSE, echo = FALSE}
# Code to knit slides
xaringan::inf_mr('_sessions/D2S3_MachineLearning/MachineLearning.Rmd')
```


```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
# see: https://github.com/yihui/xaringan
# install.packages("xaringan")
# see: 
# https://github.com/yihui/xaringan/wiki
# https://github.com/gnab/remark/wiki/Markdown
options(width=110)
options(digits = 4)
library(baselers)

library(tidyverse)
```


# What is machine learning?

.pull-left6[


### Algorithms autonomously learning from data.

Given data, an algorithm tunes its *parameters* to match the data, understand how it works, and make predictions for what will occur in the future.

```{r, echo = FALSE, out.width = "80%", fig.align = 'center'}
knitr::include_graphics("https://raw.githubusercontent.com/therbootcamp/therbootcamp.github.io/master/_sessions/_image/mldiagram_A.png")
```

]

.pull-right4[

```{r, echo = FALSE, out.width = "70%", fig.align = 'center'}
knitr::include_graphics("https://raw.githubusercontent.com/therbootcamp/therbootcamp.github.io/master/_sessions/_image/machinelearningcartoon.png")
```


]

---

# Everyone uses machine learning

.pull-left6[

How does Google know what search results you want?

How does Amazon know what products to recommend?

How does Netflix decide what shows you'll want to watch next?

How do Tesla cars recognize objects and predict accidents?

> Machine learning drives our algorithms for demand forecasting, product search ranking, product and deals recommendations, merchandising placements, fraud detection, translations, and much more. ~ Jeff Bezos, Amazon founder

]


.pull-right4[

```{r, echo = FALSE, out.width = "100%", fig.align = 'center'}
knitr::include_graphics("https://raw.githubusercontent.com/therbootcamp/therbootcamp.github.io/master/_sessions/_image/mlexamples.png")
```

]


---

# What is the basic machine learning process?

```{r, echo = FALSE, out.width = "90%"}
knitr::include_graphics("https://raw.githubusercontent.com/therbootcamp/therbootcamp.github.io/master/_sessions/_image/MLdiagram.png")
```


---

# Why do we separate training from prediction?

.pull-left4[

Just because an algorithm can fit past (training) data well, does *not* necessarily mean that it will *predict* new data well.

Anyone can come up with a model of *past* data (e.g.; stock performance, lottery winnings). Predicting future performance is much more difficult.

> "An economist is an expert who will know tomorrow why the things he predicted yesterday didn't happen today." ~ Evan Esar

]
 
.pull-right6[

```{r, echo = FALSE, out.width = "80%"}
knitr::include_graphics("../_image/prediction_collage.png")
```


]

---

# What do you think?

<font size = 5>Can you come up with a model that will perfectly match past data but is worthless in predicting future data?</font><br><br>


```{r, echo = FALSE}
set.seed(100)
past <- tibble(id = 1:5,
               sex = sample(c("m", "f"), size  = 5, replace = TRUE),
               age = round(rnorm(5, mean = 45, sd = 5), 0),
               fam_history = sample(c("Yes", "No"), size = 5, replace = TRUE),
               smoking = sample(c(TRUE, FALSE), size = 5, replace = TRUE),
               disease = sample(c(0, 1), size = 5, replace = TRUE))

present <- tibble(id = 91:95,
                  sex = sample(c("m", "f"), size  = 5, replace = TRUE),
               age = round(rnorm(5, mean = 45, sd = 5), 0),
               fam_history = sample(c("Yes", "No"), size = 5, replace = TRUE),
               smoking = sample(c(TRUE, FALSE), size = 5, replace = TRUE),
               disease = rep("?", 5))
```

.pull-left45[


### Past "Training" Data
```{r, results = 'asis', echo = FALSE}
knitr::kable(past, format = "markdown")
```

]


.pull-right45[

### Future "Test" Data
```{r, echo = FALSE}
knitr::kable(present, format = "markdown")

```

]



---

## Two types of prediction tasks

.pull-left45[

```{r, echo = FALSE, out.width = "100%", fig.align = 'center'}
knitr::include_graphics("https://raw.githubusercontent.com/therbootcamp/therbootcamp.github.io/master/_sessions/_image/classification_task.png")
```


]


.pull-right45[

```{r, echo = FALSE, out.width = "100%", fig.align = 'center'}
knitr::include_graphics("https://raw.githubusercontent.com/therbootcamp/therbootcamp.github.io/master/_sessions/_image/regression_task.png")
```

]


---

## What machine learning algorithms are there?

.pull-left55[

There are thousands of machine learning algorithms from many different fields.
  - Computer vision, natural language processing, reinforcement learning...

Wikipedia lists 57 *categories* (!) of machine learning algorithms

```{r, echo = FALSE, eval = TRUE, out.width = "80%", fig.align = 'center'}
knitr::include_graphics("https://raw.githubusercontent.com/therbootcamp/therbootcamp.github.io/master/_sessions/_image/wikipediaml.png")
```

]

.pull-right4[
<br><br>

### 3 Algorithims

We will focus on 3 algorithms that apply to most ML tasks:

| Algorithm|Complexity|
|:------|:----|
|     Decision Trees| Low |
|     Regression| Low / Medium | 
|     Random Forests| High |

]




---

# How do you fit and evaluate ML models in R?

.pull-left45[

Answer: Pretty much the same way you fit standard statistical models. Install the package, load, and find the main fitting functions.

```{r, eval = FALSE}
# Install the glmnet package
install.packages("glmnet")

# Load glmnet
library(glmnet)

# Look at help menu
?glmnet
```

Some functions will use the standard `FUN(formula, data)` arguments, but others (like `glmnet()`) require other arguments, like `x, y` (numeric matrices).

The trick is looking at the help file for additional arguments and data requirements

]

.pull-right5[

```{r, echo = FALSE}
knitr::include_graphics("../_image/glmnet_help.jpg")
```

]

---

## Regression

.pull-left5[

In regression, the criterion is modeled as the weighted sum of predictors times *weights* $\beta_{1}$, $\beta_{2}$

### Loan Default:

One could model the risk of defaulting on a loan as:

$$Risk = Age \times \beta_{age} + Income \times \beta_{income} + ...$$

Training a model means finding values of $\beta_{Age}$ and $\beta_{Income}$ that 'best' match the training data.

```{r, echo = FALSE, eval = TRUE, out.width = "50%", fig.align = 'center'}
knitr::include_graphics("https://raw.githubusercontent.com/therbootcamp/therbootcamp.github.io/master/_sessions/_image/regression.png")
```

]

.pull-right5[

### Standard regression with `glm()`

The `glm()` function in the base stats package performs standard regression.

```{r, eval = FALSE}
glm_mod <- glm(formula = happiness ~ .,
               data = baselers)
```

### Regularised regression with glmnet

To perform regularised regression, which tries to reduce overfitting by penalising large coefficients, use the `glmnet()` package:

```{r, eval = FALSE}
install.packages("glmnet")
library("glmnet")

net_mod <- glmnet(x, # Numeric features
                  y, # Response
                  alpha, # mixing parameter)
```

]

---

## Decision Trees

.pull-left45[

In decision trees, the criterion is modeled as a sequence of logical Yes or No questions.

### Loan Default:

```{r, echo = FALSE, eval = TRUE}
knitr::include_graphics("https://raw.githubusercontent.com/therbootcamp/therbootcamp.github.io/master/_sessions/_image/defaulttree.png")
```


]

.pull-right5[

### Decision trees with rpart

Create decision trees with `rpart`

```{r, eval = FALSE}
install.packages("rpart")
library(rpart)

# Train rpart model
loan_rpart_mod <- rpart(formula, data,
                        method = "class",
                        rpart.control)
```

### Fast-and-frugal trees with FFTrees

Create very simple fast-and-frugal decision trees with `FFTrees`

```{r,eval = FALSE}
install.packages("FFTrees")
library(FFTrees)

loan_FFTrees_mod <- FFTrees(formula, data,
                            max.levels, goal)
```
 

]


---

## Ensemble decision trees

.pull-left5[

A [Random Forest](https://en.wikipedia.org/wiki/Random_forest) is a collection of many (hundreds, thousands) of decision trees that use different features

```{r, echo = FALSE, eval = TRUE, out.width = "90%", fig.cap = "<font size=3><a href='https://medium.com/@williamkoehrsen'>Sourcemedium.com</a></font>"}
knitr::include_graphics("../_image/randomforest_diagram.png")
```
 
]

.pull-right5[

Create a random forest using the `randomForest` package

```{r, eval = FALSE}
install.packages("randomForest")
library(randomForest)

# Create a randomForest model
randomForest(formula = y ~.,    # Formula 
             data = data_train, # Training data
             ntree, mtry)  # Tuning parameters
```

Here are some of the tuning parameters

|Parameter | Description|
|:-------|:-------|
|`ntree`|Number of trees in forest|
|`mtry`|Number of variables randomly selected at splits|





]

---

## Optimizing model parameters with cross validation

.pull-left45[

Most machine learning models have two types of parameters, *raw parameters* that dictate how an individual model makes predictions, and *tuning parameters* that determine how those raw parameters are calculated.

|Model| Raw parameters|Tuning parameters|
|:------|:------|:------|
|Decision Trees|Nodes, splits, decisions |Minimum number of cases in each node|
|Regularised regression |Model coefficients | Coefficient penalty term|

To determine 'optimal' tuning parameters, which maximize prediction performance, techniques such as cross validation are often used.

]

.pull-right5[

### Cross Validation

```{r, echo = FALSE, out.width = "80%", fig.align = 'center'}
knitr::include_graphics("../_image/crossvalidation.jpg")
```

0) Select *tuning parameters*

1) Split the training set into K "folds"

2) Use K - 1 folds for training, and 1 fold for testing.

3) Repeat K times.

4) Average the K testing performances

]


---

.pull-left55[

# Caret

```{r, echo = FALSE, fig.align = 'center', out.width = "30%", eval = FALSE}
knitr::include_graphics("https://vignette.wikia.nocookie.net/joke-battles/images/2/21/Bugs-Bunny-4.png/revision/latest?cb=20151231234917")
```

Caret stands for <b>C</b>lassification <b>A</b>nd <b>RE</b>gression <b>T</b>raining.

`caret` is a 'wrapper' packages that automates much of the the machine learning process.

Do very complex machine learning tasks with a few simple functions

Use any of hundreds of different ML algorithms by changing one "string' (not line!) to use a different model

```{r, eval = FALSE}
library(caret)

train(..., method = "lm") # Regression!
train(..., method = "rf") # Random forests!
train(..., method = "ada") # Boosted trees
```

Knows each model's tuning parameters and chooses the best ones for your data using cross validation (or other method).

]

.pull-right4[
<br><br><br>
```{r, echo = FALSE, fig.align = 'center', out.width = "90%", fig.cap = "The almighty Caret!"}
knitr::include_graphics("https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2014/09/Caret-package-in-R.png")
```

```{r, echo = FALSE, out.width = "100%", fig.align = 'center'}
knitr::include_graphics("https://upload.wikimedia.org/wikipedia/commons/1/1c/K-fold_cross_validation_EN.jpg")
```


]


---
.pull-left5[

# Caret

```{r, echo = FALSE, message = FALSE}
library(caret)
```


As always, you can install `caret` from CRAN

```{r, eval = FALSE, echo = TRUE}
# Install caret
install.packages("caret")

# Load the caret package
library("caret")
```

Once you've installed `caret`, look at the vignette for a nice overview of the package

```{r, eval = FALSE}
# Open the main package vignette
vignette("caret")
```

Today we will go over the main functions in the package

]

.pull-right45[

### Caret Vignettes

The `caret` package has some of the *best* documentation (vignettes) you'll ever see.

```{r, echo = FALSE, out.width = "90%"}
knitr::include_graphics("caret_vignette.jpg")
```

]

---

.pull-left5[
# Caret

Here are the main functions we will cover in the `caret` package

| Function| Purpose|
|--------|----------|
| `creacreateDataPartition()` | Split data into different partitions|
| `trainControl()` | Determine how training (in general) will be done|
| `train()` | Specify a model and find *best* parameters|
| `varImp()` | Determine variable importance |

]

.pull-right45[

### Caret Vignettes

The `caret` package has some of the *best* documentation (vignettes) you'll ever see.

```{r, echo = FALSE, out.width = "90%"}
knitr::include_graphics("caret_vignette.jpg")
```

]


---

.pull-left6[
### createDataPartition()

Use `createDataPartition()` to split a dataset into separate partitions.

Useful to split a large dataset into separate training and test data

```{r, warning = FALSE}
data <- baselers %>% 
  drop_na     # Drop missing cases from baselers

# Get training cases
train_v <- createDataPartition(y = data$income, #crit
                               times = 1, 
                               p = .5)$Resample1

# Vector of training cases
train_v[1:10]

# Training Data
baselers_train <- data %>% 
  slice(train_v)

# Testing Data
baselers_test <- data %>% 
  slice(-train_v)
```


]

.pull-right35[

### Training and Test

```{r}
### Training data

dim(baselers_train)

baselers_train %>%
  slice(1:5) %>%
  select(1:3)

### Testing data

dim(baselers_test)

```


]


---
.pull-left55[
### trainControl()

Use `trainControl()` to define how `carat` should select the best parameters for any ML model (that you will specify later with `train()`)

Many methods are available in the `method` argument, look at the help menu with `?trainControl` for additional details

```{r, echo = TRUE, out.width = "90%", eval = TRUE}
# Define how caraet should 
#  select best model parameters

ctrl <- trainControl(method = "repeatedcv",
                     number = 10,  # 10 folds
                     repeats = 2) # Repeat 2 times
```

### trainControl methods

|method|Description|
|:----|:----|
|`"repeatedcv"` | Repeated cross-validation|
|`"LOOCV"`| Leave one out cross-validation|
|`"none"` | Fit one model with default parameters |

]

.pull-right4[
<br><br><br><br>
### Cross Validation
```{r, echo = FALSE, message = FALSE}
library(baselers)
knitr::include_graphics("../_image/crossvalidation.jpg")
```

]

---

.pull-left55[

### train()

Use `train()` to fit **any** of over 280 models **and** get best parameters

```{r, echo = TRUE, out.width = "90%", eval = TRUE, warning = FALSE}
rpart_train <- train(form = income ~ .,  
                     data = baselers_train,
                     method = "rpart", # Model!
                     trControl = ctrl)
```


|Parameter|Description|
|:-----|:----|
|`form`|Formula specifying criterion|
|`data`|Training data|
|`method`| Model|
|`trControl`| Control parameters|


]

.pull-right4[

See all >280 models on the [Caret Documentation Page](http://topepo.github.io/caret/available-models.html)

```{r, echo = FALSE}
knitr::include_graphics("caret_models.jpg")
```

]

---

### train()

Print the result of `train()` to obtain overall results

```{r, echo = TRUE, out.width = "90%", eval = TRUE, warning = FALSE}
rpart_train
```



---
.pull-left4[

### train()

Explore your `train` Object

```{r, echo = TRUE, out.width = "90%", eval = TRUE, warning = FALSE}
# Best tuning parameters
rpart_train$bestTune

# Show the final model
rpart_train$finalModel
```

]


.pull-right55[

```{r, fig.align = 'center'}
# Plot final model
plot(rpart_train$finalModel)
text(rpart_train$finalModel)
```


]

---

.pull-left5[

### varImp()

Use `varImp()` to extract **variable importance** from a model

Result will be be a model-specific method indicaating how important each variable was in predicting the criterion.
    
```{r}
varImp(rpart_train)
```    
    
]

.pull-right45[

```{r, fig.width = 7, fig.height = 4}
# Plot variable importance
plot(varImp(rpart_train))
```

]

---

.pull-left5[
## predict(), postResample()

Test a models prediction performance with `predict()`

```{r}
# Get predictions based on best 
bas_pred <- predict(object = rpart_train, 
                    newdata = baselers_test)

# Result is a vector of predictions 
#   for each row in newdata
bas_pred[1:5]

# Assess performance with postResample()
postResample(pred = bas_pred, 
             obs = baselers_test$income)
```



]

.pull-right45[

### Plot predictions vs Truth

```{r, fig.width = 4, fig.height = 3, fig.align = 'center'}
# Plot predictions versus truth
end <- tibble(pred = bas_pred,
              obs = baselers_test$income)

ggplot(data = results,
       aes(x = pred, y = obs)) + 
  geom_point(alpha = .1) +
  labs(title = "Rpart Performance",
       x = "Predictions",
       y = 'Truth') +
  theme_bw() +
  geom_smooth(method = "lm")
```


]

---
.pull-left5[

## Recap - ML steps with caret

Step 0: Create training and test data (if necessary)

```{r, eval = FALSE}
train_v <- createDataPartition(y, times, p)

data_train <- data %>% slice(train_v)
data_test <- data %>% slice(-train_v)
```

Step 1: Define control parameters

```{r, eval = FALSE}
ctl <- trainControl(method = "repeatedcv",
                    number = 10, 
                    repeats = 2) 
```


Step 2: Train model

```{r, eval = FALSE}
rpart_train <- train(form = income ~ .,  
                     data = data_train,
                     method = "rpart",
                     trControl = ctl)
```

]

.pull-right45[
<br><br><br>
Step 3: Explore

```{r, eval = FALSE}
rpart_train            # Print object
varImp(rpart_train)    # Var importance
rpart_train$finalModel # Final model
```


Step 4: Predict 

```{r, eval = FALSE}
my_pred <- predict(object = rpart_train, 
                   newdata = data_test)
```


Step 4: Evaluate 

```{r, eval = FALSE}
postResample(pred = bas_pred, 
             obs = baselers_test$income)
```


]


---

<br><br>
.pull-left5[

# Caret

There are *many* other great features of caret we haven't touched
- Imputing (replacing) missing values and transforming predictors with `preProcess()`
- Add your own custom model

Be sure to check out the **excellent** documentation site to learn all the details


]

.pull-right45[

### http://topepo.github.io/caret/index.html

```{r, echo = FALSE}
knitr::include_graphics("caret_package_md.jpg")
```

]

---

## Machine Learning Live Demo & Practical

<p><font size=6><b><a href="https://therbootcamp.github.io/BaselRBootcamp_2018April/_sessions/D3S1_MachineLearningII/MachineLearningII_practical.html">Link to Machine Learning II practical</a>






