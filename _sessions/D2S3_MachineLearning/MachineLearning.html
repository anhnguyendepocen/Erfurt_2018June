<!DOCTYPE html>
<html>
  <head>
    <title>Machine Learning</title>
    <meta charset="utf-8">
    <meta name="author" content="The R Bootcamp @ Erfurt www.therbootcamp.com @therbootcamp" />
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link rel="stylesheet" href="../_css/my-theme.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Machine Learning
### The R Bootcamp @ Erfurt<br/><a href='https://therbootcamp.github.io'>www.therbootcamp.com</a><br/><a href='https://twitter.com/therbootcamp'><span class="citation">@therbootcamp</span></a>
### June 2018

---


layout: true

&lt;div class="my-footer"&gt;&lt;span&gt;
&lt;a href="https://therbootcamp.github.io/"&gt;&lt;font color="#7E7E7E"&gt;Erfurt, June 2018&lt;/font&gt;&lt;/a&gt;
&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;
&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;
&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;
&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;
&lt;a href="https://therbootcamp.github.io/"&gt;&lt;font color="#646464"&gt;www.therbootcamp.com&lt;/font&gt;&lt;/a&gt;
&lt;/span&gt;&lt;/div&gt; 

---
  







# What is machine learning?

.pull-left6[


### Algorithms autonomously learning from data.

Given data, an algorithm tunes its *parameters* to match the data, understand how it works, and make predictions for what will occur in the future.

&lt;img src="https://raw.githubusercontent.com/therbootcamp/therbootcamp.github.io/master/_sessions/_image/mldiagram_A.png" width="80%" style="display: block; margin: auto;" /&gt;

]

.pull-right4[

&lt;img src="https://raw.githubusercontent.com/therbootcamp/therbootcamp.github.io/master/_sessions/_image/machinelearningcartoon.png" width="70%" style="display: block; margin: auto;" /&gt;


]

---

# Everyone uses machine learning

.pull-left6[

How does Google know what search results you want?

How does Amazon know what products to recommend?

How does Netflix decide what shows you'll want to watch next?

How do Tesla cars recognize objects and predict accidents?

&gt; Machine learning drives our algorithms for demand forecasting, product search ranking, product and deals recommendations, merchandising placements, fraud detection, translations, and much more. ~ Jeff Bezos, Amazon founder

]


.pull-right4[

&lt;img src="https://raw.githubusercontent.com/therbootcamp/therbootcamp.github.io/master/_sessions/_image/mlexamples.png" width="100%" style="display: block; margin: auto;" /&gt;

]


---

## Two types of prediction tasks

.pull-left45[

&lt;img src="https://raw.githubusercontent.com/therbootcamp/therbootcamp.github.io/master/_sessions/_image/classification_task.png" width="100%" style="display: block; margin: auto;" /&gt;


]


.pull-right45[

&lt;img src="https://raw.githubusercontent.com/therbootcamp/therbootcamp.github.io/master/_sessions/_image/regression_task.png" width="100%" style="display: block; margin: auto;" /&gt;

]

---

# What is the basic machine learning process?

&lt;img src="https://raw.githubusercontent.com/therbootcamp/therbootcamp.github.io/master/_sessions/_image/MLdiagram.png" width="90%" /&gt;


---

# Why do we separate training from prediction?

.pull-left4[

Just because an algorithm can fit past (training) data well, does *not* necessarily mean that it will *predict* new data well.

&lt;br&gt; 

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="https://raw.githubusercontent.com/therbootcamp/therbootcamp.github.io/master/_sessions/_image/stockpen.jpg" alt="Anyone can come up with a model of past stock performance. Predicting future performance is much more difficult." width="70%" /&gt;
&lt;p class="caption"&gt;Anyone can come up with a model of past stock performance. Predicting future performance is much more difficult.&lt;/p&gt;
&lt;/div&gt;

]

.pull-right6[

&gt; "Prediction is difficult, especially when it is about the future" ~ Niels Bohr

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="https://raw.githubusercontent.com/therbootcamp/therbootcamp.github.io/master/_sessions/_image/bohr.jpg" alt="Niels Bohr, Nobel Laureate in Physics" width="20%" /&gt;
&lt;p class="caption"&gt;Niels Bohr, Nobel Laureate in Physics&lt;/p&gt;
&lt;/div&gt;

&gt; "An economist is an expert who will know tomorrow why the things he predicted yesterday didn't happen today." ~ Evan Esar

&lt;!-- &gt; "A prediction about the direction of the stock market tells you nothing about where stocks are headed, but a whole lot about the person doing the predicting" ~ Warren Buffett --&gt;

]

---

# What do you think?

&lt;font size = 5&gt;Can anyone come up with a model that will perfectly match past data but is worthless in predicting future data?&lt;/font&gt;&lt;br&gt;&lt;br&gt;




.pull-left45[


### Past "Training" Data

| id|sex | age|fam_history |smoking | disease|
|--:|:---|---:|:-----------|:-------|-------:|
|  1|m   |  46|Yes         |TRUE    |       0|
|  2|m   |  47|Yes         |FALSE   |       1|
|  3|f   |  42|No          |FALSE   |       1|
|  4|m   |  49|No          |FALSE   |       1|
|  5|m   |  41|No          |TRUE    |       1|
|  6|m   |  43|Yes         |FALSE   |       0|
|  7|f   |  45|Yes         |FALSE   |       1|
|  8|m   |  45|No          |TRUE    |       1|

]


.pull-right45[

### Future "Test" Data

| id|sex | age|fam_history |smoking |disease |
|--:|:---|---:|:-----------|:-------|:-------|
| 91|m   |  39|No          |TRUE    |?       |
| 92|m   |  46|No          |FALSE   |?       |
| 93|m   |  45|No          |FALSE   |?       |
| 94|m   |  54|No          |FALSE   |?       |
| 95|m   |  44|No          |TRUE    |?       |
| 96|m   |  44|No          |FALSE   |?       |
| 97|f   |  42|No          |FALSE   |?       |
| 98|m   |  44|Yes         |TRUE    |?       |

]


---

## What machine learning algorithms are there?

.pull-left55[

There are thousands of machine learning algorithms from many different fields.
  - Computer vision, natural language processing, reinforcement learning...

Wikipedia lists 57 *categories* (!) of machine learning algorithms

&lt;img src="https://raw.githubusercontent.com/therbootcamp/therbootcamp.github.io/master/_sessions/_image/wikipediaml.png" width="80%" style="display: block; margin: auto;" /&gt;

]

.pull-right4[
&lt;br&gt;&lt;br&gt;

### 3 Algorithims

We will focus on 3 algorithms that apply to most ML tasks:

| Algorithm|Complexity|
|:------|:----|
|     Decision Trees| Low |
|     Regression| Low / Medium | 
|     Random Forests| High |

]




---

.pull-left6[

## How do you fit and evaluate models in R?

&lt;br&gt;

| Step|Description| Note / Example |
|:------|:---|:------------|
| 1| Install model packages| `FFTrees` for Decision Trees&lt;br&gt;`randomForest` for Random Forests|
|     2| Get data |Use your own, or get free online datasets|
|    3| Train model on data and generate insights|Always look at help menus and online tutorials!|
|    4| Predict new data, possibly with cross-validation|Packages such as `mlr` and `caret` can really help|

]

.pull-right35[
&lt;br&gt;
&lt;img src="https://raw.githubusercontent.com/therbootcamp/therbootcamp.github.io/master/_sessions/_image/machinelearning_r_ss.png" width="90%" style="display: block; margin: auto;" /&gt;

]

---

# How do you fit and evaluate models in R?

.pull-left45[

### Fitting a model



```r
A_model &lt;- A_fun(formula = y ~.,
                 data = data_train,
                 ...)
```


| Argument| Description| Note |
|------:|:----|:---|
|     formula|  Formula indicating variables to use|  `y ~ .` is often used as a catch-all |
|     data|    The dataset for model training| |
|     ...|  Optional other arguments| See the function help page for details|


]

.pull-right5[


### Evaluating a model


```r
# Common ways to explore / use a model

A_model           # Print generic information

names(A_model)    # Show attributes

summary(A_model)  # Print summary information

predict(A_model,  # Predict test data
        newdata = data_test)  

plot(A_model)     # Visualize the model
```

]

---

## Regression with `glm()`

.pull-left5[

In regression, the criterion is modeled as the weighted sum of predictors times *weights* `\(\beta_{1}\)`, `\(\beta_{2}\)`

### Loan Default:

One could model the risk of defaulting on a loan as:

`$$Risk = Age \times \beta_{age} + Income \times \beta_{income} + ...$$`

Training a model means finding values of `\(\beta_{Age}\)` and `\(\beta_{Income}\)` that 'best' match the training data.

&lt;img src="https://raw.githubusercontent.com/therbootcamp/therbootcamp.github.io/master/_sessions/_image/regression.png" width="50%" style="display: block; margin: auto;" /&gt;


]


.pull-right5[

Create regressions using the `glm()` function (part of base-R)


```r
# glm() function for regression
glm(formula = y ~.,     # Formula
    data = data_train,  # Training data
    family, ...)        # Optional arguments

# Train glm model
loan_glm_model &lt;- glm(formula = risk ~ ., 
                      data = data_train)

# Predict new data with glm model
loan_glm_pred &lt;- predict(loan_glm_model,
                         newdata = data_test)
```

### Regularised regression with glmnet

We will also play with 'regularised' ridge regression with the `glmnet` package

]


---

## Decision Trees with `rpart::rpart()`

.pull-left45[

In decision trees, the criterion is modeled as a sequence of logical Yes or No questions.

### Loan Default:

![](https://raw.githubusercontent.com/therbootcamp/therbootcamp.github.io/master/_sessions/_image/defaulttree.png)&lt;!-- --&gt;


]

.pull-right5[

Create decision trees using the `rpart` package


```r
# Load the rpart package
library(rpart)

# Train rpart model
loan_rpart_mod &lt;- rpart(formula = risk ~ ., 
                        data = loan_data,
                        rpart.control)

# Predict new data with rpart model
loan_rpart_pred &lt;- predict(loan_rpart_mod,
                           newdata = loan_test)
```

### Fast-and-frugal trees with FFTrees

We will also play with 'fast-and-frugal trees' with the `FFTrees` package]

---

## Random Forests with `randomForest::randomForest()`

.pull-left5[

A Random Forest is a collection of many (hundreds, thousands) of decision trees


&lt;img src="https://raw.githubusercontent.com/therbootcamp/therbootcamp.github.io/master/_sessions/_image/randomforest_diagram.png" width="90%" /&gt;


]

.pull-right5[

Create decision trees using the `randomForest` package


```r
# Load the randomforest package
library(randomForest)

# Calculating a randomForest in R
randomForest(formula = y ~.,    # Formula 
             data = data_train, # Training data
             ntree, mtry)       # Optional

# Train randomForest model
rf_model &lt;- randomForest(formula = risk ~ ., 
                         data = loan_data)

# Predict new data with model
rf_pred &lt;- predict(rf_model,
                   newdata = loan_test)
```


]


---

# 2 Ways to do machine learning in R

.pull-left45[

### Raw method

Call individual packages and functions directly


```r
# LOAD LIBRARIES
library(randomForest)
library(ada)
library(elasticnet)

# Train models on training data
rf_mod &lt;- randomForest(...)
boost_mod &lt;- ada(...)
rlr_mod &lt;- ridge(...)

# Predictions
rf_pred &lt;- predict(rf_mod, ...)
boost_pred &lt;- predict(rf_mod, ...)

#...
```

]

.pull-right5[

### Meta Method 

Use a "meta" package like `caret` (or `mlr`):

`caret` is a 'wrapper' packages that automates much of the the machine learning process.

Use hundreds of different ML algorithms using a common language and set of functions. Change one "string' (not line!) to use a different model


```r
library(caret)

train(..., method = "lm") # Regression!
train(..., method = "rf") # Random forests!
train(..., method = "ada") # Boosted regression trees
```


&lt;img src="../_image/caretlogo.jpeg" width="80%" style="display: block; margin: auto;" /&gt;

Benefit: Makes it *extremely* easy to perform very complex machine learning procedures in a few lines of code


]

---

.pull-left55[

# Caret


&lt;img src="https://vignette.wikia.nocookie.net/joke-battles/images/2/21/Bugs-Bunny-4.png/revision/latest?cb=20151231234917" width="35%" style="display: block; margin: auto;" /&gt;

Caret stands for Classification And REgression Training.

Do very complex machine learning tasks with a few simple functions

Knows each model's meta parameters and chooses the best ones with cross validation.

]

.pull-right45[
&lt;br&gt;&lt;br&gt;&lt;br&gt;
&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2014/09/Caret-package-in-R.png" alt="The almighty Caret!" width="90%" /&gt;
&lt;p class="caption"&gt;The almighty Caret!&lt;/p&gt;
&lt;/div&gt;

&lt;img src="https://upload.wikimedia.org/wikipedia/commons/1/1c/K-fold_cross_validation_EN.jpg" width="100%" style="display: block; margin: auto;" /&gt;


]


---
.pull-left5[

# Caret


As always, you can install `caret` from CRAN


```r
# Install caret
install.packages("caret")

# Load the caret package
library("caret")
```

Once you've installed `caret`, look at the vignette for a nice overview of the package


```r
# Open the main package vignette
vignette("caret")
```


Today we will go over the main functions in the package


]

.pull-right45[

### Caret Vignette

&lt;img src="caret_vignette.jpg" width="90%" /&gt;


]


---

.pull-left5[
# Caret


Here are the main functions in the `caret` package

| Function| Purpose|
|--------|----------|
| `trainControl()` | Determine how training (in general) will be done|
| `train()` | Specify a model and find *best* parameters |
| `varImp()` | Determine variable importance |

]


.pull-right45[

### Caret Vignette

&lt;img src="caret_vignette.jpg" width="90%" /&gt;

]

---

# Caret

### trainControl()

- Use `trainControl()` to define a general fitting process.



```r
# Get ready for 10 fold cross validation!

mycontrol &lt;- trainControl(method = "repeatedcv", # Do repeated cross validation
                          number = 10,           # 10 folds
                          repeats = 50)         # Repeat 50 times (!)
```


- Can specify cross validation with `method = "repeatedcv"`
    - Many other methods are available too!
    
- Can *repeat* cross validation many times with `repeats`

---

.pull-left6[

# Caret

### train()

- Use `train()` to fit **any** of over 200 models **and** get best parameters


```r
train(form = price ~ .,      # Criterion
      data = data_train,     # Training data
      method = "lm",         # Specify a model
      trControl = mycontrol) # Use mycontrol parameters
```

- Specify the criterion to be predicted as a formula with `form`

- Specify training data with `data`

- Choose a model with `method`
    - `method = 'lm'` = Linear regression
    - `method = 'rf` = Random forest

]

.pull-right35[

See all &gt;280 models at [http://topepo.github.io/caret/available-models.html](http://topepo.github.io/caret/available-models.html)

&lt;img src="caret_models.jpg" width="1699" /&gt;


]

---

.pull-left5[

# Caret

### varImp()

- Use `varImp()` to extract **variable importance** from a model


```r
# Look at variable importance with varImp
varImp(diamonds_lm_train)
```

- Result will be a vector showing how important each variable was in predicting the criterion.

- Specific outputs depend on the model
    - Regression: Regression weights
    - Decision trees: Mean *gini* index.
    
]


.pull-right45[

![](varimp_ss.jpg)&lt;!-- --&gt;


]


---


&lt;br&gt;&lt;br&gt;
.pull-left5[

# Caret


- There are *many* other great features of caret we haven't touched
   - Splitting data with `createDataPartition()`
   - Imputing (replacing) missing values and transforming predictors with `preProcess()`
   - Add your own custom model
   

- Be sure to check out the **excellent** documentation site to learn all the details


]

.pull-right45[

### http://topepo.github.io/caret/index.html


&lt;img src="caret_package_md.jpg" width="2251" /&gt;

]

---

## Machine Learning II Live Demo &amp; Practical

&lt;p&gt;&lt;font size=6&gt;&lt;b&gt;&lt;a href="https://therbootcamp.github.io/BaselRBootcamp_2018April/_sessions/D3S1_MachineLearningII/MachineLearningII_practical.html"&gt;Link to Machine Learning II practical&lt;/a&gt;






---

## Machine Learning Live Demo &amp; Practical

&lt;p&gt;&lt;font size=6&gt;&lt;b&gt;&lt;a href="https://therbootcamp.github.io/BaselRBootcamp_2018April/_sessions/D2S3_MachineLearning/MachineLearning_practical.html"&gt;Link to Machine Learning practical&lt;/a&gt;


&lt;!-- --- --&gt;
&lt;!-- # What is the history of machine learning? --&gt;

&lt;!-- - 1805 - 1809: Legendre and Gauss discover least squares. Soon after Galton defines **Regression** in a biological context, followed by Pearson for purely statistical analyses. --&gt;

&lt;!-- - 1952: Arthur Samuel creates first computer learning program for learning checkers and coins the term **Machine Learning** in 1959.  --&gt;

&lt;!-- - 1957: Frank Rosenblatt creates first **Neural Network** to simulate the thought process of the human brain. --&gt;

&lt;!-- - 1963: First algorithm for **Support Vector Machines** is developed by Vapnik &amp; Chervonenkis. --&gt;

&lt;!-- - 1967: **Nearest neighbor algorithm** is developed for classification --&gt;

&lt;!-- - 1984: Breiman &amp; Olshen publish the CART algorithm for **Decision Trees**, followed by Quinlan who publishes the ID3 algorithm followed by C4.5 --&gt;

&lt;!-- - 1986: Rina Dechter introduces **Deep Learning**, with many subsequent updates in the 2000s. --&gt;

&lt;!-- - 1995: Tin Kam Ho develops first algorithm for **Random Forests** --&gt;

&lt;!-- Sources: Wikipedia, Bernard Marr, "A Short History of Machine Learning", Forbes. --&gt;



---
### Old




---
# Why do we separate training from prediction?

- Data comes from two processes: *Signal* and *Noise* (aka Error).
&lt;br&gt;

&lt;img src="MachineLearning_files/figure-html/unnamed-chunk-39-1.png" width="80%" style="display: block; margin: auto;" /&gt;




---
# Why do we separate training from prediction?

- A good model is one that tries to capture the signal and ignore the noise
- A bad model is one that captures too much unpredictable noise,
    

&lt;img src="MachineLearning_files/figure-html/unnamed-chunk-40-1.png" width="80%" style="display: block; margin: auto;" /&gt;
    </textarea>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function() {
  var d = document, s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})();</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
  }
});
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
